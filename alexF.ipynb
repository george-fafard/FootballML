{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sportsipy in /home/aleigator/.local/lib/python3.9/site-packages (0.6.0)\n",
      "Requirement already satisfied: pandas>=0.24.1 in /home/aleigator/.local/lib/python3.9/site-packages (from sportsipy) (1.2.1)\n",
      "Requirement already satisfied: requests>=2.18.4 in /usr/lib/python3.9/site-packages (from sportsipy) (2.25.1)\n",
      "Requirement already satisfied: pyquery>=1.4.0 in /home/aleigator/.local/lib/python3.9/site-packages (from sportsipy) (1.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3.9/site-packages (from pandas>=0.24.1->sportsipy) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/aleigator/.local/lib/python3.9/site-packages (from pandas>=0.24.1->sportsipy) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/aleigator/.local/lib/python3.9/site-packages (from pandas>=0.24.1->sportsipy) (1.20.0)\n",
      "Requirement already satisfied: cssselect>0.7.9 in /home/aleigator/.local/lib/python3.9/site-packages (from pyquery>=1.4.0->sportsipy) (1.1.0)\n",
      "Requirement already satisfied: lxml>=2.1 in /home/aleigator/.local/lib/python3.9/site-packages (from pyquery>=1.4.0->sportsipy) (4.6.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=0.24.1->sportsipy) (1.15.0)\n",
      "Requirement already satisfied: chardet>=3.0.2 in /usr/lib/python3.9/site-packages (from requests>=2.18.4->sportsipy) (3.0.4)\n",
      "Requirement already satisfied: idna>=2.5 in /usr/lib/python3.9/site-packages (from requests>=2.18.4->sportsipy) (2.10)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/lib/python3.9/site-packages (from requests>=2.18.4->sportsipy) (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sportsipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sportsipy.nfl.boxscore import Boxscores, Boxscore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sklearn utilities\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# sklearn models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_data(game_df,game_stats):\n",
    "    try:\n",
    "        away_team_df = game_df[['away_name', 'away_abbr', 'away_score']].rename(columns = {'away_name': 'team_name', 'away_abbr': 'team_abbr', 'away_score': 'score'})\n",
    "        home_team_df = game_df[['home_name','home_abbr', 'home_score']].rename(columns = {'home_name': 'team_name', 'home_abbr': 'team_abbr', 'home_score': 'score'})\n",
    "        try:\n",
    "            if game_df.loc[0,'away_score'] > game_df.loc[0,'home_score']:\n",
    "                away_team_df = pd.merge(away_team_df, pd.DataFrame({'game_won' : [1], 'game_lost' : [0]}),left_index = True, right_index = True)\n",
    "                home_team_df = pd.merge(home_team_df, pd.DataFrame({'game_won' : [0], 'game_lost' : [1]}),left_index = True, right_index = True)\n",
    "            elif game_df.loc[0,'away_score'] < game_df.loc[0,'home_score']:\n",
    "                away_team_df = pd.merge(away_team_df, pd.DataFrame({'game_won' : [0], 'game_lost' : [1]}),left_index = True, right_index = True)\n",
    "                home_team_df = pd.merge(home_team_df, pd.DataFrame({'game_won' : [1], 'game_lost' : [0]}),left_index = True, right_index = True)\n",
    "            else: \n",
    "                away_team_df = pd.merge(away_team_df, pd.DataFrame({'game_won' : [0], 'game_lost' : [0]}),left_index = True, right_index = True)\n",
    "                home_team_df = pd.merge(home_team_df, pd.DataFrame({'game_won' : [0], 'game_lost' : [0]}),left_index = True, right_index = True)\n",
    "        except TypeError:\n",
    "                away_team_df = pd.merge(away_team_df, pd.DataFrame({'game_won' : [np.nan], 'game_lost' : [np.nan]}),left_index = True, right_index = True)\n",
    "                home_team_df = pd.merge(home_team_df, pd.DataFrame({'game_won' : [np.nan], 'game_lost' : [np.nan]}),left_index = True, right_index = True)        \n",
    "\n",
    "        away_stats_df = game_stats.dataframe[['away_first_downs', 'away_fourth_down_attempts',\n",
    "               'away_fourth_down_conversions', 'away_fumbles', 'away_fumbles_lost',\n",
    "               'away_interceptions', 'away_net_pass_yards', 'away_pass_attempts',\n",
    "               'away_pass_completions', 'away_pass_touchdowns', 'away_pass_yards',\n",
    "               'away_penalties', 'away_points', 'away_rush_attempts',\n",
    "               'away_rush_touchdowns', 'away_rush_yards', 'away_third_down_attempts',\n",
    "               'away_third_down_conversions', 'away_time_of_possession',\n",
    "               'away_times_sacked', 'away_total_yards', 'away_turnovers',\n",
    "               'away_yards_from_penalties', 'away_yards_lost_from_sacks']].reset_index().drop(columns ='index').rename(columns = {\n",
    "               'away_first_downs': 'first_downs', 'away_fourth_down_attempts':'fourth_down_attempts',\n",
    "               'away_fourth_down_conversions':'fourth_down_conversions' , 'away_fumbles': 'fumbles', 'away_fumbles_lost': 'fumbles_lost',\n",
    "               'away_interceptions': 'interceptions', 'away_net_pass_yards':'net_pass_yards' , 'away_pass_attempts': 'pass_attempts',\n",
    "               'away_pass_completions':'pass_completions' , 'away_pass_touchdowns': 'pass_touchdowns', 'away_pass_yards': 'pass_yards',\n",
    "               'away_penalties': 'penalties', 'away_points': 'points', 'away_rush_attempts': 'rush_attempts',\n",
    "               'away_rush_touchdowns': 'rush_touchdowns', 'away_rush_yards': 'rush_yards', 'away_third_down_attempts': 'third_down_attempts',\n",
    "               'away_third_down_conversions': 'third_down_conversions', 'away_time_of_possession': 'time_of_possession',\n",
    "               'away_times_sacked': 'times_sacked', 'away_total_yards': 'total_yards', 'away_turnovers': 'turnovers',\n",
    "               'away_yards_from_penalties':'yards_from_penalties', 'away_yards_lost_from_sacks': 'yards_lost_from_sacks'})\n",
    "\n",
    "        home_stats_df = game_stats.dataframe[['home_first_downs', 'home_fourth_down_attempts',\n",
    "               'home_fourth_down_conversions', 'home_fumbles', 'home_fumbles_lost',\n",
    "               'home_interceptions', 'home_net_pass_yards', 'home_pass_attempts',\n",
    "               'home_pass_completions', 'home_pass_touchdowns', 'home_pass_yards',\n",
    "               'home_penalties', 'home_points', 'home_rush_attempts',\n",
    "               'home_rush_touchdowns', 'home_rush_yards', 'home_third_down_attempts',\n",
    "               'home_third_down_conversions', 'home_time_of_possession',\n",
    "               'home_times_sacked', 'home_total_yards', 'home_turnovers',\n",
    "               'home_yards_from_penalties', 'home_yards_lost_from_sacks']].reset_index().drop(columns = 'index').rename(columns = {\n",
    "               'home_first_downs': 'first_downs', 'home_fourth_down_attempts':'fourth_down_attempts',\n",
    "               'home_fourth_down_conversions':'fourth_down_conversions' , 'home_fumbles': 'fumbles', 'home_fumbles_lost': 'fumbles_lost',\n",
    "               'home_interceptions': 'interceptions', 'home_net_pass_yards':'net_pass_yards' , 'home_pass_attempts': 'pass_attempts',\n",
    "               'home_pass_completions':'pass_completions' , 'home_pass_touchdowns': 'pass_touchdowns', 'home_pass_yards': 'pass_yards',\n",
    "               'home_penalties': 'penalties', 'home_points': 'points', 'home_rush_attempts': 'rush_attempts',\n",
    "               'home_rush_touchdowns': 'rush_touchdowns', 'home_rush_yards': 'rush_yards', 'home_third_down_attempts': 'third_down_attempts',\n",
    "               'home_third_down_conversions': 'third_down_conversions', 'home_time_of_possession': 'time_of_possession',\n",
    "               'home_times_sacked': 'times_sacked', 'home_total_yards': 'total_yards', 'home_turnovers': 'turnovers',\n",
    "               'home_yards_from_penalties':'yards_from_penalties', 'home_yards_lost_from_sacks': 'yards_lost_from_sacks'})\n",
    "\n",
    "        away_team_df = pd.merge(away_team_df, away_stats_df,left_index = True, right_index = True)\n",
    "        home_team_df = pd.merge(home_team_df, home_stats_df,left_index = True, right_index = True)\n",
    "        try:\n",
    "            away_team_df['time_of_possession'] = (int(away_team_df['time_of_possession'].loc[0][0:2]) * 60) + int(away_team_df['time_of_possession'].loc[0][3:5])\n",
    "            home_team_df['time_of_possession'] = (int(home_team_df['time_of_possession'].loc[0][0:2]) * 60) + int(home_team_df['time_of_possession'].loc[0][3:5])\n",
    "        except TypeError:\n",
    "            away_team_df['time_of_possession'] = np.nan\n",
    "            home_team_df['time_of_possession'] = np.nan\n",
    "    except TypeError:\n",
    "        away_team_df = pd.DataFrame()\n",
    "        home_team_df = pd.DataFrame()\n",
    "    return away_team_df, home_team_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_data_up_to_week(weeks,year):\n",
    "    weeks_games_df = pd.DataFrame()\n",
    "    for w in range(len(weeks)):\n",
    "        date_string = str(weeks[w]) + '-' + str(year)\n",
    "        week_scores = Boxscores(weeks[w],year)\n",
    "        week_games_df = pd.DataFrame()\n",
    "        for g in range(len(week_scores.games[date_string])):\n",
    "            game_str = week_scores.games[date_string][g]['boxscore']\n",
    "            game_stats = Boxscore(game_str)\n",
    "            game_df = pd.DataFrame(week_scores.games[date_string][g], index = [0])\n",
    "            away_team_df, home_team_df = game_data(game_df,game_stats)\n",
    "            away_team_df['week'] = weeks[w]\n",
    "            home_team_df['week'] = weeks[w]\n",
    "            week_games_df = pd.concat([week_games_df,away_team_df])\n",
    "            week_games_df = pd.concat([week_games_df,home_team_df])\n",
    "        weeks_games_df = pd.concat([weeks_games_df,week_games_df])\n",
    "    return weeks_games_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1995 = game_data_up_to_week(range(1, 16), 1995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2000 = game_data_up_to_week(range(1, 16), 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2005 = game_data_up_to_week(range(1, 16), 2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2010 = game_data_up_to_week(range(1, 16), 2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015 = game_data_up_to_week(range(1, 16), 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = game_data_up_to_week(range(1,16), 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1995 Season\n",
    "y_1995 = df_1995[\"game_won\"]\n",
    "x_1995 = df_1995[list(df_1995.columns)[5:len(df_1995)]]\n",
    "\n",
    "# 2000 Season\n",
    "y_2000 = df_2000[\"game_won\"]\n",
    "x_2000 = df_2000[list(df_2000.columns)[5:len(df_2000)]]\n",
    "\n",
    "# 2005 Season\n",
    "y_2005 = df_2005[\"game_won\"]\n",
    "x_2005 = df_2005[list(df_2005.columns)[5:len(df_2005)]]\n",
    "\n",
    "# 2010 Season\n",
    "y_2010 = df_2010[\"game_won\"]\n",
    "x_2010 = df_2010[list(df_2010.columns)[5:len(df_2010)]]\n",
    "\n",
    "# 2015 Season\n",
    "y_2015 = df_2015[\"game_won\"]\n",
    "x_2015 = df_2015[list(df_2015.columns)[5:len(df_2015)]]\n",
    "\n",
    "# 2019 Season\n",
    "y_2019 = df_2019[\"game_won\"]\n",
    "x_2019 = df_2019[list(df_2019.columns)[5:len(df_2019)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1995 Season Data split\n",
    "x_1995_train, x_1995_test, y_1995_train, y_1995_test = train_test_split(x_1995, y_1995)\n",
    "\n",
    "# 2000 Season Data split\n",
    "x_2000_train, x_2000_test, y_2000_train, y_2000_test = train_test_split(x_2000, y_2000)\n",
    "\n",
    "# 2005 Season Data split\n",
    "x_2005_train, x_2005_test, y_2005_train, y_2005_test = train_test_split(x_2005, y_2005)\n",
    "\n",
    "# 2010 Season Data split\n",
    "x_2010_train, x_2010_test, y_2010_train, y_2010_test = train_test_split(x_2010, y_2010)\n",
    "\n",
    "# 2015 Season Data split\n",
    "x_2015_train, x_2015_test, y_2015_train, y_2015_test = train_test_split(x_2015, y_2015)\n",
    "\n",
    "# 2019 Season Data split\n",
    "x_2019_train, x_2019_test, y_2019_train, y_2019_test = train_test_split(x_2019, y_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenny\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\lenny\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\lenny\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\lenny\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\lenny\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\lenny\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=750)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up model for 1995 season\n",
    "clf_1995 = LogisticRegression(max_iter=750)\n",
    "clf_1995.fit(x_1995_train, y_1995_train)\n",
    "\n",
    "# Set up model for 2000 season\n",
    "clf_2000 = LogisticRegression(max_iter=750)\n",
    "clf_2000.fit(x_2000_train, y_2000_train)\n",
    "\n",
    "# Set up model for 2005 season\n",
    "clf_2005 = LogisticRegression(max_iter=750)\n",
    "clf_2005.fit(x_2005_train, y_2005_train)\n",
    "\n",
    "# Set up model for 2010 season\n",
    "clf_2010 = LogisticRegression(max_iter=750)\n",
    "clf_2010.fit(x_2010_train, y_2010_train)\n",
    "\n",
    "# Set up model for 2015 season\n",
    "clf_2015 = LogisticRegression(max_iter=750)\n",
    "clf_2015.fit(x_2015_train, y_2015_train)\n",
    "\n",
    "# Set up model for 2019 season\n",
    "clf_2019 = LogisticRegression(max_iter=750)\n",
    "clf_2019.fit(x_2019_train, y_2019_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995 score: 0.8285714285714286\n",
      "2000 score: 0.8532110091743119\n",
      "2005 score: 0.8303571428571429\n",
      "2010 score: 0.875\n",
      "2015 score: 0.8035714285714286\n",
      "2019 score: 0.8392857142857143\n"
     ]
    }
   ],
   "source": [
    "# Predict 1995 games based on 1995 model\n",
    "d_1995 = clf_1995.predict(x_1995_test)\n",
    "print(\"1995 score:\", clf_1995.score(x_1995_test, y_1995_test))\n",
    "\n",
    "# Predict 2000 games based on 2000 model\n",
    "d_2000 = clf_2000.predict(x_2000_test)\n",
    "print(\"2000 score:\", clf_2000.score(x_2000_test, y_2000_test))\n",
    "\n",
    "# Predict 2005 games based on 2005 model\n",
    "d_2005 = clf_2005.predict(x_2005_test)\n",
    "print(\"2005 score:\", clf_2005.score(x_2005_test, y_2005_test))\n",
    "\n",
    "# Predict 2010 games based on 2010 model\n",
    "d_2010 = clf_2010.predict(x_2010_test)\n",
    "print(\"2010 score:\", clf_2010.score(x_2010_test, y_2010_test))\n",
    "\n",
    "# Predict 2015 games based on 2015 model\n",
    "d_2015 = clf_2015.predict(x_2015_test)\n",
    "print(\"2015 score:\", clf_2015.score(x_2015_test, y_2015_test))\n",
    "\n",
    "# Predict 2019 games based on 2019 model\n",
    "d_2019 = clf_2019.predict(x_2019_test)\n",
    "print(\"2019 score:\", clf_2019.score(x_2019_test, y_2019_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_precision_recall(conf_matrix):\n",
    "    # conf matrix composition:\n",
    "    # by definition a confusion matrix C\n",
    "    # is such that C i,j is equal to the number of observations known to be in group i and predicted to be in group j.\n",
    "    # get our true positives\n",
    "    rows, cols = np.shape(conf_matrix)\n",
    "    # we will need to track these\n",
    "    tp_array = []\n",
    "    fp_array = []\n",
    "    fn_array = []\n",
    "    p_array = []\n",
    "    r_array = []\n",
    "    # so the i,j value where i = j should be our true positives\n",
    "    # our false negative should be every other case in a row,\n",
    "    # our false positive should be every other case in a column.\n",
    "    for i in range(0, rows):\n",
    "        row_sum = 0\n",
    "        col_sum = 0\n",
    "        for j in range(0, cols):\n",
    "            if i == j:\n",
    "                tp_array.append(conf_matrix[i, j])\n",
    "            else:\n",
    "                row_sum += conf_matrix[j,i]\n",
    "                col_sum += conf_matrix[i,j]\n",
    "        fp_array.append(row_sum)\n",
    "        fn_array.append(col_sum)\n",
    "    \n",
    "    for k in range(0, cols):\n",
    "        # precision = tp / (tp + fp)\n",
    "        p_array.append(tp_array[k]/(tp_array[k] + fp_array[k]))\n",
    "        # recall = tp / (tp + fn)\n",
    "        r_array.append(tp_array[k]/(tp_array[k] + fn_array[k]))\n",
    "    \n",
    "    metrix = pd.DataFrame(p_array, columns=['Precision'])\n",
    "    metrix['Recall'] = r_array\n",
    "    return metrix\n",
    "def f1_score(conf_matrix):\n",
    "    # get some metrics\n",
    "    metrix = custom_precision_recall(conf_matrix)\n",
    "    p_array = []\n",
    "    r_array = []\n",
    "    f1_array = []\n",
    "    # calculate the F1 score\n",
    "    for p in metrix['Precision']:\n",
    "        p_array.append(p)\n",
    "    for r in metrix['Recall']:\n",
    "        r_array.append(r)\n",
    "    \n",
    "    # F1 score = 2 * PR / (P + R)\n",
    "    for i in range(0, len(p_array)):\n",
    "        f1_array.append(2 * p_array[i] * r_array[i] / (p_array[i] + r_array[i]))\n",
    "    return pd.DataFrame(f1_array, columns=[\"F1 Score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1995 = confusion_matrix(d_1995, y_1995_test)\n",
    "cm_df1995 = pd.DataFrame(cm1995, columns = [\"loss\", 'win'])\n",
    "\n",
    "cm2000 = confusion_matrix(d_2000, y_2000_test)\n",
    "cm_df2000 = pd.DataFrame(cm2000, columns = [\"loss\", 'win'])\n",
    "\n",
    "cm2005 = confusion_matrix(d_2005, y_2005_test)\n",
    "cm_df2005 = pd.DataFrame(cm2005, columns = [\"loss\", 'win'])\n",
    "\n",
    "cm2010 = confusion_matrix(d_2010, y_2010_test)\n",
    "cm_df2010 = pd.DataFrame(cm2010, columns = [\"loss\", 'win'])\n",
    "\n",
    "cm2015 = confusion_matrix(d_2015, y_2015_test)\n",
    "cm_df2015 = pd.DataFrame(cm2015, columns = [\"loss\", 'win'])\n",
    "\n",
    "cm2019 = confusion_matrix(d_2019, y_2019_test)\n",
    "cm_df2019 = pd.DataFrame(cm2019, columns = [\"loss\", 'win'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   loss  win\n",
      "0    43    5\n",
      "1    13   44\n",
      "\n",
      "   Precision    Recall\n",
      "0   0.767857  0.895833\n",
      "1   0.897959  0.771930\n",
      "\n",
      "   F1 Score\n",
      "0  0.826923\n",
      "1  0.830189\n",
      "\n",
      "   loss  win\n",
      "0    47    8\n",
      "1     8   46\n",
      "\n",
      "   Precision    Recall\n",
      "0   0.854545  0.854545\n",
      "1   0.851852  0.851852\n",
      "\n",
      "   F1 Score\n",
      "0  0.854545\n",
      "1  0.851852\n",
      "\n",
      "   loss  win\n",
      "0    45    6\n",
      "1    13   48\n",
      "\n",
      "   Precision    Recall\n",
      "0   0.775862  0.882353\n",
      "1   0.888889  0.786885\n",
      "\n",
      "   F1 Score\n",
      "0  0.825688\n",
      "1  0.834783\n",
      "\n",
      "   loss  win\n",
      "0    52    8\n",
      "1     6   46\n",
      "\n",
      "   Precision    Recall\n",
      "0   0.896552  0.866667\n",
      "1   0.851852  0.884615\n",
      "\n",
      "   F1 Score\n",
      "0  0.881356\n",
      "1  0.867925\n",
      "\n",
      "   loss  win\n",
      "0    42    8\n",
      "1    14   48\n",
      "\n",
      "   Precision    Recall\n",
      "0   0.750000  0.840000\n",
      "1   0.857143  0.774194\n",
      "\n",
      "   F1 Score\n",
      "0  0.792453\n",
      "1  0.813559\n",
      "\n",
      "   loss  win\n",
      "0    47    5\n",
      "1    13   47\n",
      "\n",
      "   Precision    Recall\n",
      "0   0.783333  0.903846\n",
      "1   0.903846  0.783333\n",
      "\n",
      "   F1 Score\n",
      "0  0.839286\n",
      "1  0.839286\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(str(cm_df1995) + \"\\n\")\n",
    "print(str(custom_precision_recall(cm1995)) + \"\\n\")\n",
    "print(str(f1_score(cm1995)) + \"\\n\")\n",
    "\n",
    "print(str(cm_df2000) + \"\\n\")\n",
    "print(str(custom_precision_recall(cm2000)) + \"\\n\")\n",
    "print(str(f1_score(cm2000)) + \"\\n\")\n",
    "\n",
    "print(str(cm_df2005) + \"\\n\")\n",
    "print(str(custom_precision_recall(cm2005)) + \"\\n\")\n",
    "print(str(f1_score(cm2005)) + \"\\n\")\n",
    "\n",
    "print(str(cm_df2010) + \"\\n\")\n",
    "print(str(custom_precision_recall(cm2010)) + \"\\n\")\n",
    "print(str(f1_score(cm2010)) + \"\\n\")\n",
    "\n",
    "print(str(cm_df2015) + \"\\n\")\n",
    "print(str(custom_precision_recall(cm2015)) + \"\\n\")\n",
    "print(str(f1_score(cm2015)) + \"\\n\")\n",
    "\n",
    "print(str(cm_df2019) + \"\\n\")\n",
    "print(str(custom_precision_recall(cm2019)) + \"\\n\")\n",
    "print(str(f1_score(cm2019)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
